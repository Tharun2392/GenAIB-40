{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tharun2392/GenAIB-40/blob/main/2303A52386_WEEK4_ASSIGNMENT_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backpropagation with Linear Activation Function"
      ],
      "metadata": {
        "id": "O1fdC62mUcIZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkSchbJ2KHgc",
        "outputId": "36445753-bc3a-4b37-d4ff-d50425eadcb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100, Error: 0.034809611071508144\n",
            "Epoch 200, Error: 0.02725368186653142\n",
            "Epoch 300, Error: 0.022038455760788\n",
            "Epoch 400, Error: 0.018532890031437696\n",
            "Epoch 500, Error: 0.01645016718560968\n",
            "Epoch 600, Error: 0.015237143130072177\n",
            "Epoch 700, Error: 0.014730045955688085\n",
            "Epoch 800, Error: 0.014757990633368249\n",
            "Epoch 900, Error: 0.01479566042200329\n",
            "Epoch 1000, Error: 0.01478835209422752\n",
            "\n",
            "Test Predictions:\n",
            "[[0.47294867]\n",
            " [0.53803768]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "# Training data (Table 1)\n",
        "X_train = np.array([[0.1, 0.2, 0.3],\n",
        "                    [0.2, 0.3, 0.4],\n",
        "                    [0.3, 0.4, 0.5],\n",
        "                    [0.5, 0.6, 0.7],\n",
        "                    [0.1, 0.3, 0.5],\n",
        "                    [0.2, 0.4, 0.6],\n",
        "                    [0.3, 0.5, 0.7],\n",
        "                    [0.4, 0.6, 0.8],\n",
        "                    [0.5, 0.7, 0.1]])\n",
        "\n",
        "y_train = np.array([[0.14],\n",
        "                    [0.20],\n",
        "                    [0.26],\n",
        "                    [0.38],\n",
        "                    [0.22],\n",
        "                    [0.28],\n",
        "                    [0.34],\n",
        "                    [0.40],\n",
        "                    [0.22]])\n",
        "\n",
        "# Parameters\n",
        "input_size = X_train.shape[1]\n",
        "output_size = 1\n",
        "learning_rate = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "# Initialize weights and biases\n",
        "np.random.seed(42)\n",
        "W = np.random.randn(input_size, output_size)\n",
        "b = np.zeros((1, output_size))\n",
        "\n",
        "# Linear activation function and its derivative\n",
        "def linear(x):\n",
        "    return x\n",
        "\n",
        "def linear_derivative(x):\n",
        "    return np.ones_like(x)\n",
        "\n",
        "# Training the ANN\n",
        "for epoch in range(epochs):\n",
        "    # Forward propagation\n",
        "    z = np.dot(X_train, W) + b\n",
        "    a = linear(z)\n",
        "\n",
        "    # Calculate the error\n",
        "    error = y_train - a\n",
        "\n",
        "    # Backward propagation\n",
        "    d_a = error * linear_derivative(a)\n",
        "    d_W = np.dot(X_train.T, d_a)\n",
        "    d_b = np.sum(d_a, axis=0, keepdims=True)\n",
        "\n",
        "    # Update weights and biases\n",
        "    W += learning_rate * d_W\n",
        "    b += learning_rate * d_b\n",
        "\n",
        "    # Print the error at every 100th epoch\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f'Epoch {epoch + 1}, Error: {np.mean(np.abs(error))}')\n",
        "\n",
        "# Testing the ANN with the test data (Table 2)\n",
        "X_test = np.array([[0.6, 0.7, 0.8],\n",
        "                   [0.7, 0.8, 0.9]])\n",
        "\n",
        "y_test = np.array([[0.44],\n",
        "                   [0.50]])\n",
        "\n",
        "z_test = np.dot(X_test, W) + b\n",
        "a_test = linear(z_test)\n",
        "\n",
        "print(\"\\nTest Predictions:\")\n",
        "print(a_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backpropagation with Sigmoid Activation Function"
      ],
      "metadata": {
        "id": "VyNf2MkHUTVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Training data (Table 1)\n",
        "X_train = np.array([\n",
        "    [0.1, 0.2, 0.3],\n",
        "    [0.2, 0.3, 0.4],\n",
        "    [0.3, 0.4, 0.5],\n",
        "    [0.5, 0.6, 0.7],\n",
        "    [0.1, 0.3, 0.5],\n",
        "    [0.2, 0.4, 0.6],\n",
        "    [0.3, 0.5, 0.7],\n",
        "    [0.4, 0.6, 0.8],\n",
        "    [0.5, 0.7, 0.1],\n",
        "])\n",
        "y_train = np.array([0.14, 0.20, 0.26, 0.38, 0.22, 0.28, 0.34, 0.40, 0.22])\n",
        "\n",
        "# Test data (Table 2)\n",
        "X_test = np.array([\n",
        "    [0.6, 0.7, 0.8],\n",
        "    [0.7, 0.8, 0.9],\n",
        "])\n",
        "y_test = np.array([0.44, 0.50])\n",
        "\n",
        "# Initialize weights and bias\n",
        "weights = np.random.rand(3)  # One weight for each input feature\n",
        "bias = np.random.rand()\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "def predict(X):\n",
        "    return np.dot(X, weights) + bias\n",
        "\n",
        "# Training the model using gradient descent\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    y_pred = predict(X_train)\n",
        "\n",
        "    # Compute the error\n",
        "    error = y_pred - y_train\n",
        "\n",
        "    # Calculate gradients\n",
        "    weight_gradients = np.dot(X_train.T, error) / len(X_train)\n",
        "    bias_gradient = np.mean(error)\n",
        "\n",
        "    # Update weights and bias\n",
        "    weights -= learning_rate * weight_gradients\n",
        "    bias -= learning_rate * bias_gradient\n",
        "\n",
        "# Mean squared error function\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "# Evaluate on training and test data\n",
        "train_mse = mean_squared_error(y_train, predict(X_train))\n",
        "test_mse = mean_squared_error(y_test, predict(X_test))\n",
        "\n",
        "print(\"Training MSE:\", train_mse)\n",
        "print(\"Testing MSE:\", test_mse)\n",
        "\n",
        "# Predicting user input data\n",
        "user_input = input(\"Enter values for x1, x2, x3 separated by spaces: \")\n",
        "user_data = np.array(list(map(float, user_input.split()))).reshape(1, -1)  # Reshape to 2D array\n",
        "user_prediction = predict(user_data)\n",
        "print(\"Predicted output:\", user_prediction[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLAJJdTJUUcG",
        "outputId": "33943439-77d8-4e4f-88ea-ab8ec0e366ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training MSE: 0.007698218855630061\n",
            "Testing MSE: 0.03623252405811181\n",
            "Enter values for x1, x2, x3 separated by spaces: 0.2 0.3 0.4\n",
            "Predicted output: 0.2820243462838857\n"
          ]
        }
      ]
    }
  ]
}